"""
main.py - ë©€í‹°ëª¨ë‹¬ í•„í„° ì§€ì› Active Learning ì‹¤í—˜ ì‹¤í–‰ê¸°
"""

import os

# ==========================================
# CUDA ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì • (ê°€ì¥ ë¨¼ì € ì‹¤í–‰)
# ==========================================
# CUDA ë©”ëª¨ë¦¬ ë‹¨í¸í™” ë°©ì§€ ë° íš¨ìœ¨ì  í• ë‹¹ ì„¤ì •
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128,expandable_segments:True'

# GPU ì„¤ì •
# os.environ['CUDA_VISIBLE_DEVICES'] = '2'  # GPU 2ë²ˆ ì‚¬ìš©

import time
import traceback
from config import ExperimentConfig
from utils import check_dependencies, get_model_files, Timer
from active_learning import YOLOActiveLearning

class ExperimentRunner:
    """ì‹¤í—˜ ì‹¤í–‰ê¸°"""
    
    def __init__(self, check_deps=True):
        self.timer = Timer()
        
        if check_deps:
            try:
                check_dependencies()
                print("âœ“ ì˜ì¡´ì„± í™•ì¸ ì™„ë£Œ")
            except ImportError as e:
                print(f"âš ï¸ ì˜ì¡´ì„± ê²½ê³ : {e}")
                print("âš ï¸ ì¼ë¶€ ê¸°ëŠ¥ì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
    
    def run_experiment(self, config: ExperimentConfig, skip_cycle_0=False):
        """ë‹¨ì¼ ì‹¤í—˜ ì‹¤í–‰"""
        print(f"\n{'='*80}")
        print("Active Learning ì‹¤í—˜ ì‹œì‘")
        print(f"{'='*80}")
        print(config.get_summary())
        print(f"Cycle 0 ê±´ë„ˆë›°ê¸°: {skip_cycle_0}")
        
        # ì„¤ì • ê²€ì¦
        try:
            config.validate()
            print("âœ“ ì„¤ì • ê²€ì¦ ì™„ë£Œ")
        except ValueError as e:
            print(f"âœ— ì„¤ì • ì˜¤ë¥˜: {e}")
            return False
        
        # ëª¨ë¸ íŒŒì¼ í™•ì¸
        yolo_models = get_model_files(config.models_dir, ".pt")
        if not yolo_models:
            print(f"âœ— YOLO ëª¨ë¸ ì—†ìŒ: {config.models_dir}")
            return False
        
        # ë¶„ë¥˜ê¸° í™•ì¸
        classifier_models = []
        
        if config.use_multimodal_filter:
            print("âœ“ ë©€í‹°ëª¨ë‹¬ í•„í„° ì‚¬ìš© - VLM ë° CNN ëª¨ë¸ ìë™ ë‹¤ìš´ë¡œë“œ")
            print(f"  - VLM: {config.multimodal_vlm_type}")
            print(f"  - CNN: DenseNet121")
            print(f"  - í‚¤ì›Œë“œ: {config.target_keywords}")
            print(f"  - í•™ìŠµ ìƒ˜í”Œ: í´ë˜ìŠ¤ë‹¹ {config.multimodal_train_samples}ê°œ")
            classifier_models = [None]
            
        elif config.use_captioning_classifier:
            print("âœ“ ìº¡ì…”ë‹ ë¶„ë¥˜ê¸° ì‚¬ìš© - ì‚¬ì „í›ˆë ¨ ëª¨ë¸ ìë™ ë‹¤ìš´ë¡œë“œ")
            classifier_models = [None]
            
        elif config.use_classifier:
            classifier_models = get_model_files(config.classifiers_dir, ".pth")
            if not classifier_models:
                print(f"âœ— ë¶„ë¥˜ ëª¨ë¸ ì—†ìŒ: {config.classifiers_dir}")
                return False
            
            for clf_path in classifier_models:
                if not os.path.exists(clf_path):
                    print(f"âœ— ë¶„ë¥˜ ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {clf_path}")
                    return False
                else:
                    print(f"âœ“ ë¶„ë¥˜ ëª¨ë¸ í™•ì¸: {os.path.basename(clf_path)}")
        else:
            classifier_models = [None]
        
        print(f"âœ“ YOLO ëª¨ë¸ {len(yolo_models)}ê°œ")
        
        if config.use_multimodal_filter:
            print(f"âœ“ ë©€í‹°ëª¨ë‹¬ í•„í„°: VLM={config.multimodal_vlm_type}, CNN=DenseNet121")
        elif config.use_captioning_classifier:
            print(f"âœ“ ìº¡ì…”ë‹ ë¶„ë¥˜ê¸°: {config.captioning_model_type}, í‚¤ì›Œë“œ: {config.target_keywords}")
        elif config.use_classifier:
            print(f"âœ“ ë¶„ë¥˜ ëª¨ë¸ {len(classifier_models)}ê°œ")
        
        # ì‹¤í—˜ ì‹¤í–‰
        self.timer.start()
        success_count = 0
        total_count = 0
        
        for classifier_path in classifier_models:
            # ë¶„ë¥˜ê¸° ì´ë¦„
            if config.use_multimodal_filter:
                classifier_name = f"multimodal_{config.multimodal_vlm_type}"
            elif config.use_captioning_classifier:
                classifier_name = f"captioning_{config.captioning_model_type}"
            elif classifier_path is None:
                classifier_name = "no_classifier"
            else:
                classifier_name = os.path.splitext(os.path.basename(classifier_path))[0]
            
            for model_path in yolo_models:
                model_name = os.path.splitext(os.path.basename(model_path))[0]
                total_count += 1
                
                cycle_info = "Cycle 1ë¶€í„°" if skip_cycle_0 else "Cycle 0ë¶€í„°"
                print(f"\n--- ì‹¤í—˜ {total_count}: {model_name} + {classifier_name} ({cycle_info}) ---")
                
                try:
                    # ê°œë³„ ì‹¤í—˜ ì„¤ì •
                    experiment_config = ExperimentConfig(**config.__dict__)
                    experiment_config.output_dir = os.path.join(
                        config.output_dir,
                        f"{classifier_name}_{model_name}"
                    )
                    
                    # Active Learning ì‹¤í–‰
                    al = YOLOActiveLearning(experiment_config, model_path, classifier_path)
                    al.run(skip_cycle_0=skip_cycle_0)
                    
                    success_count += 1
                    print(f"âœ“ ì‹¤í—˜ ì™„ë£Œ: {model_name}")
                    
                except Exception as e:
                    print(f"âœ— ì‹¤í—˜ ì‹¤íŒ¨: {str(e)}")
                    
                    # ì˜¤ë¥˜ ë¡œê·¸
                    error_dir = os.path.join(
                        config.output_dir,
                        f"{classifier_name}_{model_name}",
                        "error_logs"
                    )
                    os.makedirs(error_dir, exist_ok=True)
                    
                    with open(os.path.join(error_dir, "error.log"), "w") as f:
                        f.write(f"ì˜¤ë¥˜ ë°œìƒ: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                        f.write(f"ì˜¤ë¥˜: {str(e)}\n\n")
                        f.write(f"ìƒì„¸:\n{traceback.format_exc()}")
        
        # ì‹¤í—˜ ì™„ë£Œ
        total_time = self.timer.end()
        
        print(f"\n{'='*80}")
        print("ì‹¤í—˜ ì™„ë£Œ!")
        print(f"ì„±ê³µ: {success_count}/{total_count}")
        print(f"ì´ ì†Œìš” ì‹œê°„: {total_time/60:.1f}ë¶„")
        print(f"ê²°ê³¼: {config.output_dir}")
        print(f"{'='*80}")
        
        return success_count > 0

def main():
    """ë©”ì¸ ì‹¤í—˜ ì‹¤í–‰ í•¨ìˆ˜"""
    
    # GPU ì •ë³´ ì¶œë ¥
    import torch
    print(f"\n{'='*60}")
    print("GPU ì„¤ì • ì •ë³´")
    print(f"{'='*60}")
    print(f"CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES', 'Not set')}")
    print(f"CUDA Available: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"Current CUDA Device: {torch.cuda.current_device()}")
        print(f"Device Name: {torch.cuda.get_device_name(0)}")  # 0ì€ ê°€ì‹œì ì¸ ì²« ë²ˆì§¸ ë””ë°”ì´ìŠ¤
        print(f"Total GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
    print(f"{'='*60}\n")
    
    # ==========================================
    # ì‹¤í—˜ íŒŒë¼ë¯¸í„° ì„¤ì •
    # ==========================================
    
    models_dir = "../model weights/YOLO/coco_vehicle"
    classifiers_dir = "../model weights/Classification/visdrone"
    image_dir = "../data/car_object_detection/images"
    label_dir = "../data/car_object_detection/labels"
    output_dir = "../experiment_results/kaggle_multi_modal"
    
    # ê¸°ë³¸ íŒŒë¼ë¯¸í„°
    conf_threshold = 0.25
    iou_threshold = 0.5
    class_conf_threshold = 0.5
    max_cycles = 10
    gpu_num = 2 
    
    # ==========================================
    # ë¶„ë¥˜ê¸° ì„¤ì • (3ê°€ì§€ ì¤‘ 1ê°œë§Œ ì„ íƒ)
    # ==========================================
    
    # 1. ê¸°ì¡´ ë¶„ë¥˜ê¸°
    use_classifier = False
    enable_classifier_retraining = False
    
    # 2. ìº¡ì…”ë‹ ë¶„ë¥˜ê¸°
    use_captioning_classifier = False
    captioning_model_type = "vit-gpt2"  # "blip" ë˜ëŠ” "vit-gpt2"
    
    # 3. ë©€í‹°ëª¨ë‹¬ í•„í„° (NEW!)
    use_multimodal_filter = True

    # VLM ëª¨ë¸ ì„ íƒ (ì•„ë˜ ì¤‘ í•˜ë‚˜ ì„ íƒ)
    multimodal_vlm_type = "qwen-vl"  # ì¶”ì²œ ëª¨ë¸ ì˜µì…˜:
    # - "blip": ê°€ë³ê³  ë¹ ë¦„ (ê¸°ë³¸)
    # - "vit-gpt2": BLIP ëŒ€ì•ˆ, ë¹„ìŠ·í•œ ì„±ëŠ¥
    # - "instructblip": ìƒì„¸í•œ ì„¤ëª…, 7B ëª¨ë¸ (VRAM ìš”êµ¬ëŸ‰ ë†’ìŒ)
    # - "llava": ë©€í‹°ëª¨ë‹¬ ëŒ€í™”í˜• ëª¨ë¸, 7B (VRAM ìš”êµ¬ëŸ‰ ë†’ìŒ)
    # - "qwen-vl": Qwen ê¸°ë°˜ VLM, ìƒì„¸ ì„¤ëª… ì§€ì› âœ“ ì¶”ì²œ!

    multimodal_train_samples = 100  # í´ë˜ìŠ¤ë‹¹ IoU ê¸°ë°˜ í•™ìŠµ ìƒ˜í”Œ ìˆ˜

    # ë©€í‹°ëª¨ë‹¬ Target/Non-target ë¶„ë¥˜ IoU ì„ê³„ê°’
    multimodal_iou_threshold = 0.5  # Target: â‰¥0.5, Non-target: <0.5
    # ê¶Œì¥ê°’:
    # - 0.3: ë§¤ìš° ê´€ëŒ€ (ë” ë§ì€ Target ìƒ˜í”Œ)
    # - 0.4: ê´€ëŒ€ (ì¶©ë¶„í•œ Target ìƒ˜í”Œ)
    # - 0.5: í‘œì¤€ (COCO ê¸°ì¤€, ê¶Œì¥) âœ“
    # - 0.6: ì—„ê²© (ê³ í’ˆì§ˆ Targetë§Œ)
    # - 0.7: ë§¤ìš° ì—„ê²© (ìµœê³  í’ˆì§ˆë§Œ, Target ë¶€ì¡± ê°€ëŠ¥)

    # ìº¡ì…˜ ì €ì¥ ì„¤ì •
    save_captions = True  # VLMì´ ìƒì„±í•œ ê°ì²´ ì„¤ëª…ì„ JSON íŒŒì¼ë¡œ ì €ì¥
    captions_output_dir = None  # Noneì´ë©´ output_dir/captions ì‚¬ìš©

    # ê³µí†µ ì„¤ì •
    target_keywords = ["car", "vehicle"]  # ì–‘ì„± ê°ì²´ í‚¤ì›Œë“œ
    
    # ê¸°íƒ€ ì„¤ì •
    skip_cycle_0 = False  # Cycle 0 ê±´ë„ˆë›°ê¸°
    
    yolo_epochs = 50
    yolo_batch_size = 16
    yolo_patience = 10
    
    classifier_epochs = 20
    classifier_batch_size = 16
    max_samples_per_class = 500
    
    # ë©€í‹°ëª¨ë‹¬ í•™ìŠµ íŒŒë¼ë¯¸í„°
    multimodal_epochs = 20
    multimodal_batch_size = 16
    multimodal_learning_rate = 0.001
    
    global_seed = 42
    
    # ==========================================
    # ì„¤ì • ê²€ì¦
    # ==========================================
    
    active_classifiers = sum([use_classifier, use_captioning_classifier, use_multimodal_filter])
    if active_classifiers > 1:
        print("âœ— ì˜¤ë¥˜: í•˜ë‚˜ì˜ ë¶„ë¥˜ê¸°ë§Œ í™œì„±í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
        print("  - use_classifier")
        print("  - use_captioning_classifier")
        print("  - use_multimodal_filter")
        return
    
    if use_multimodal_filter:
        valid_vlm = ["blip", "vit-gpt2", "instructblip", "llava", "qwen-vl"]
        if multimodal_vlm_type not in valid_vlm:
            print(f"âœ— ì˜¤ë¥˜: ë©€í‹°ëª¨ë‹¬ VLMì€ {valid_vlm} ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤")
            return

        if not target_keywords:
            print("âœ— ì˜¤ë¥˜: ë©€í‹°ëª¨ë‹¬ í•„í„° ì‚¬ìš© ì‹œ target_keywords í•„ìš”")
            return

        print(f"\nâœ“ ë©€í‹°ëª¨ë‹¬ í•„í„° ì„¤ì •:")
        print(f"   - VLM ëª¨ë¸: {multimodal_vlm_type}")
        print(f"   - CNN ëª¨ë¸: DenseNet121")
        print(f"   - í‚¤ì›Œë“œ: {target_keywords}")
        print(f"   - í•™ìŠµ ìƒ˜í”Œ: í´ë˜ìŠ¤ë‹¹ {multimodal_train_samples}ê°œ (IoU ê¸°ë°˜)")
        print(f"   - IoU ì„ê³„ê°’: {multimodal_iou_threshold}")
        print(f"     Â· Target: IoU â‰¥ {multimodal_iou_threshold}")
        print(f"     Â· Non-target: IoU < {multimodal_iou_threshold}")
        print(f"   - ìº¡ì…˜ ì €ì¥: {'í™œì„±í™”' if save_captions else 'ë¹„í™œì„±í™”'}")
        if save_captions:
            caption_dir = captions_output_dir if captions_output_dir else os.path.join(output_dir, "captions")
            print(f"   - ìº¡ì…˜ ì €ì¥ ìœ„ì¹˜: {caption_dir}")
        print(f"   - Cycle 1ì—ì„œ GTì™€ IoU ë¹„êµí•˜ì—¬ ìë™ ë¶„ë¥˜")
    
    elif use_captioning_classifier:
        valid_models = ["blip", "vit-gpt2"]
        if captioning_model_type not in valid_models:
            print(f"âœ— ì˜¤ë¥˜: ìº¡ì…”ë‹ ëª¨ë¸ì€ {valid_models} ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤")
            return
        
        if not target_keywords:
            print("âœ— ì˜¤ë¥˜: ìº¡ì…”ë‹ ë¶„ë¥˜ê¸° ì‚¬ìš© ì‹œ target_keywords í•„ìš”")
            return
        
        print(f"âœ“ ìº¡ì…”ë‹ ë¶„ë¥˜ê¸° ì„¤ì •:")
        print(f"   - ëª¨ë¸: {captioning_model_type}")
        print(f"   - í‚¤ì›Œë“œ: {target_keywords}")
    
    # ==========================================
    # ë¼ë²¨ í™•ì¸
    # ==========================================
    
    labels_available = False
    if os.path.exists(label_dir):
        try:
            label_files = [f for f in os.listdir(label_dir) if f.endswith('.txt')]
            labels_available = len(label_files) > 0
        except:
            labels_available = False
    
    if not labels_available:
        print(f"\nâš ï¸ ë¼ë²¨ ì—†ìŒ: {label_dir}")
        if use_multimodal_filter:
            print("âœ— ë©€í‹°ëª¨ë‹¬ í•„í„°ëŠ” Cycle 1ì—ì„œ IoU ê¸°ë°˜ í•™ìŠµì„ ìœ„í•´ ë¼ë²¨ì´ í•„ìˆ˜ì…ë‹ˆë‹¤")
            return
        print("âš ï¸ ì„±ëŠ¥ í‰ê°€ ì—†ì´ ì§„í–‰ë©ë‹ˆë‹¤")
        
        response = input("\nê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower().strip()
        if response != 'y':
            print("ì·¨ì†Œë¨")
            return
    else:
        label_count = len([f for f in os.listdir(label_dir) if f.endswith('.txt')])
        print(f"âœ“ ë¼ë²¨: {label_count}ê°œ")
        
        if use_multimodal_filter:
            print(f"âœ“ ë©€í‹°ëª¨ë‹¬ í•„í„°: Cycle 1ì—ì„œ IoU ê¸°ë°˜ í•™ìŠµ ë°ì´í„° ìƒì„± ê°€ëŠ¥")
    
    # ==========================================
    # ì‹¤í—˜ ì„¤ì • ìƒì„±
    # ==========================================
    
    config = ExperimentConfig(
        models_dir=models_dir,
        classifiers_dir=classifiers_dir,
        image_dir=image_dir,
        label_dir=label_dir,
        output_dir=output_dir,
        labels_available=labels_available,
        gpu_num=gpu_num,
        conf_threshold=conf_threshold,
        iou_threshold=iou_threshold,
        class_conf_threshold=class_conf_threshold,
        max_cycles=max_cycles,
        max_samples_per_class=max_samples_per_class,
        use_classifier=use_classifier,
        enable_classifier_retraining=enable_classifier_retraining,
        use_captioning_classifier=use_captioning_classifier,
        captioning_model_type=captioning_model_type,
        use_multimodal_filter=use_multimodal_filter,
        multimodal_vlm_type=multimodal_vlm_type,
        multimodal_train_samples=multimodal_train_samples,
        multimodal_iou_threshold=multimodal_iou_threshold,
        save_captions=save_captions,
        captions_output_dir=captions_output_dir,
        target_keywords=target_keywords,
        yolo_epochs=yolo_epochs,
        yolo_batch_size=yolo_batch_size,
        yolo_patience=yolo_patience,
        classifier_epochs=classifier_epochs,
        classifier_batch_size=classifier_batch_size,
        multimodal_epochs=multimodal_epochs,
        multimodal_batch_size=multimodal_batch_size,
        multimodal_learning_rate=multimodal_learning_rate,
        global_seed=global_seed
    )
    
    # ==========================================
    # ì‹¤í—˜ ì‹¤í–‰
    # ==========================================
    
    print("\nYOLO Active Learning with Multimodal Filter")
    print("="*60)
    print(f"Cycle 0 ê±´ë„ˆë›°ê¸°: {skip_cycle_0}")
    
    if use_multimodal_filter:
        print(f"ğŸ§  ë©€í‹°ëª¨ë‹¬ í•„í„°:")
        print(f"   - VLM: {multimodal_vlm_type}")
        print(f"   - CNN: DenseNet121")
        print(f"   - í‚¤ì›Œë“œ: {target_keywords}")
        print(f"   - IoU ì„ê³„ê°’: {multimodal_iou_threshold}")
        print(f"   - í•™ìŠµ: Cycle 1ì—ì„œ IoU ê¸°ë°˜ ìƒ˜í”Œë§")
    elif use_captioning_classifier:
        print(f"ğŸ”¤ ìº¡ì…”ë‹ ë¶„ë¥˜ê¸°: {captioning_model_type}")
        print(f"ğŸ¯ í‚¤ì›Œë“œ: {target_keywords}")
    elif use_classifier:
        print(f"ğŸ§  ê¸°ì¡´ ë¶„ë¥˜ê¸°: ì¬í•™ìŠµ {'í™œì„±í™”' if enable_classifier_retraining else 'ë¹„í™œì„±í™”'}")
    else:
        print("ğŸš« ë¶„ë¥˜ê¸° ì—†ìŒ")
    
    if skip_cycle_0:
        print("âš¡ ë¹ ë¥¸ ëª¨ë“œ: Cycle 1ë¶€í„° ì‹œì‘")
    else:
        print("ğŸ“Š í‘œì¤€ ëª¨ë“œ: Cycle 0 ë² ì´ìŠ¤ë¼ì¸ í¬í•¨")
    
    try:
        runner = ExperimentRunner(check_deps=True)
        success = runner.run_experiment(config, skip_cycle_0=skip_cycle_0)
        
        if success:
            print("\nğŸ‰ ì‹¤í—˜ ì„±ê³µ!")
        else:
            print("\nâŒ ì‹¤í—˜ ì‹¤íŒ¨")
            
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ì‚¬ìš©ì ì¤‘ë‹¨")
    except Exception as e:
        print(f"\nğŸ’¥ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}")
        print("\nìƒì„¸:")
        print(traceback.format_exc())

if __name__ == "__main__":
    main()
